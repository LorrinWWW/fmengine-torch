{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da4251c-ffea-4eca-852e-1387b16d34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e93bca-c99f-412c-9c8c-1c1e76bdd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear.reset_parameters = lambda x: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed9f39f-9842-4c24-bc58-7330d102f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb505f35-b75a-47e7-9c88-bdbcc45f2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/home/jue/v2/CodeLlama-34b/'\n",
    "src_lora_path = '/home/jue/v2/model_ckpts/lora_delay/global_step200'\n",
    "tgt_path = '/home/jue/v2/CodeLlama-34b-delay-fine-tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d703eb52-cea5-4fd3-b45e-38cd604833b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jue/v2/CodeLlama-34b-delay-fine-tuned/tokenizer_config.json',\n",
       " '/home/jue/v2/CodeLlama-34b-delay-fine-tuned/special_tokens_map.json',\n",
       " '/home/jue/v2/CodeLlama-34b-delay-fine-tuned/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(src_path).save_pretrained(tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f10dd67-e154-4e12-8411-8687a5bfd645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7c3f1d307d4e7493802ccaae64ee15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(src_path, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7467ca1b-13ea-4916-aad7-a0e76695cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 / 48..\n",
      "layer 1 / 48..\n",
      "layer 2 / 48..\n",
      "layer 3 / 48..\n",
      "layer 4 / 48..\n",
      "layer 5 / 48..\n",
      "layer 6 / 48..\n",
      "layer 7 / 48..\n",
      "layer 8 / 48..\n",
      "layer 9 / 48..\n",
      "layer 10 / 48..\n",
      "layer 11 / 48..\n",
      "layer 12 / 48..\n",
      "layer 13 / 48..\n",
      "layer 14 / 48..\n",
      "layer 15 / 48..\n",
      "layer 16 / 48..\n",
      "layer 17 / 48..\n",
      "layer 18 / 48..\n",
      "layer 19 / 48..\n",
      "layer 20 / 48..\n",
      "layer 21 / 48..\n",
      "layer 22 / 48..\n",
      "layer 23 / 48..\n",
      "layer 24 / 48..\n",
      "layer 25 / 48..\n",
      "layer 26 / 48..\n",
      "layer 27 / 48..\n",
      "layer 28 / 48..\n",
      "layer 29 / 48..\n",
      "layer 30 / 48..\n",
      "layer 31 / 48..\n",
      "layer 32 / 48..\n",
      "layer 33 / 48..\n",
      "layer 34 / 48..\n",
      "layer 35 / 48..\n",
      "layer 36 / 48..\n",
      "layer 37 / 48..\n",
      "layer 38 / 48..\n",
      "layer 39 / 48..\n",
      "layer 40 / 48..\n",
      "layer 41 / 48..\n",
      "layer 42 / 48..\n",
      "layer 43 / 48..\n",
      "layer 44 / 48..\n",
      "layer 45 / 48..\n",
      "layer 46 / 48..\n",
      "layer 47 / 48..\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.model.layers)):\n",
    "    print(f'layer {i} / {len(model.model.layers)}..')\n",
    "    layer = model.model.layers[i]\n",
    "    for j in range(4):\n",
    "        state_dict = torch.load(\n",
    "            f'{src_lora_path}/layer_{i+1:02d}-model_{j:02d}-model_states.pt',\n",
    "            map_location='cuda:0'\n",
    "        )\n",
    "        q_lora = (state_dict['self_attn.q_proj.lora_B'] @ state_dict['self_attn.q_proj.lora_A'])\n",
    "        layer.self_attn.q_proj.weight.data[j*q_lora.size(0):(j+1)*q_lora.size(0)] += q_lora.cpu() # * (alpha / r)\n",
    "\n",
    "        v_lora = (state_dict['self_attn.v_proj.lora_B'] @ state_dict['self_attn.v_proj.lora_A'])\n",
    "        layer.self_attn.v_proj.weight.data[j*v_lora.size(0):(j+1)*v_lora.size(0)] += v_lora.cpu() # * (alpha / r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bb3fed-afc4-471c-8668-35ce84850f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(tgt_path, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7a45da-7641-45df-b654-d6cac54fd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75d9dc-c4fc-4fea-8ace-a015860818fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
